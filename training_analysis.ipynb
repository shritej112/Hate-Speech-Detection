{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzXF66kW5Qng",
        "outputId": "f16a968a-b216-407c-ccd0-f951c1e1c2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "NCf0Q2xz12WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_dir = '/kaggle/input/nlp-data'\n",
        "# output_dir = '/kaggle/working'\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/NLP/data'\n",
        "output_dir = '/content/drive/MyDrive/NLP'\n"
      ],
      "metadata": {
        "id": "saHPtktv5mtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_sent_train = pd.read_csv(f'{data_dir}/marathi/L3Cube-MahaNLP/Sentiment/tweets-train.csv')\n",
        "mr_sent_test = pd.read_csv(f'{data_dir}/marathi/L3Cube-MahaNLP/Sentiment/tweets-test.csv')\n",
        "mr_sent_val = pd.read_csv(f'{data_dir}/marathi/L3Cube-MahaNLP/Sentiment/tweets-valid.csv')\n",
        "\n",
        "mr_sent = pd.concat([mr_sent_train, mr_sent_val, mr_sent_test], axis=0, ignore_index=True)\n",
        "mr_sent.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "mr_sent = mr_sent.sample(frac=1)\n",
        "print(\"Marathi Sentiment Dataset:\")\n",
        "print(mr_sent.head())\n",
        "\n",
        "\n",
        "\n",
        "mr_hate_offn_train = pd.read_excel(f'{data_dir}/marathi/L3Cube-MahaNLP/HateOffensive/hate_train.xlsx')\n",
        "mr_hate_offn_test = pd.read_excel(f'{data_dir}/marathi/L3Cube-MahaNLP/HateOffensive/hate_test.xlsx')\n",
        "mr_hate_offn_val = pd.read_excel(f'{data_dir}/marathi/L3Cube-MahaNLP/HateOffensive/hate_valid.xlsx')\n",
        "\n",
        "mr_hate_offn = pd.concat([mr_hate_offn_train, mr_hate_offn_val, mr_hate_offn_test], axis=0, ignore_index=True)\n",
        "mr_hate_offn = mr_hate_offn.sample(frac=1)\n",
        "print(\"\\nMarathi and Offensive Speech Dataset:\")\n",
        "print(mr_hate_offn.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIf4uqV5z7my",
        "outputId": "c0960fff-0ca7-4bb6-df44-6db0a4b4834b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marathi Sentiment Dataset:\n",
            "                                                    text  label\n",
            "14684    आज घराघरात पारंपारिक गुढीसोबतच कोरोनाविरूद्ध...      1\n",
            "1655    @PRaimule आज  हमीभाव 2775 असताना\\nसोयाबीनचा भ...      1\n",
            "2512     सनातन  संस्थेच्या हिटलिस्टवर जितेंद्र आव्हाड...      0\n",
            "11624    उध्दव आजोबा, तोंड सांभाळून बोला\\netvbharat.p...     -1\n",
            "11219    सा. बां. विभागाकडून #HAM (#HybridAnnuity) अं...      0\n",
            "\n",
            "Marathi and Offensive Speech Dataset:\n",
            "                                                    text label\n",
            "4347   ग्राहक साथ देतील पण दलिंदर सेवा कर्मचारी देतात...  HATE\n",
            "5445   @indlramesh @BhausahebAjabe @manjrekarmahesh त...  OFFN\n",
            "13606  @PrashantPGurav1 @HiteshASalvi @vikassh_14 @ek...  PRFN\n",
            "17575  जाहिर निषेध व्यक्त करण्या पेक्षा महाराष्ट्राती...   NOT\n",
            "18597  कोविड-19 लस बूस्टर्स सीडीसी व्हायरसच्या गंभीर ...   NOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn_sent_train = pd.read_csv(f'{data_dir}/bangla/SAIL/BN_data_train.tsv', sep='\\t')\n",
        "bn_sent_test = pd.read_csv(f'{data_dir}/bangla/SAIL/BN_data_test.tsv', sep='\\t')\n",
        "bn_sent_val = pd.read_csv(f'{data_dir}/bangla/SAIL/BN_data_dev.tsv', sep='\\t')\n",
        "\n",
        "bn_sent = pd.concat([bn_sent_train, bn_sent_val, bn_sent_test], axis=0, ignore_index=True)\n",
        "bn_sent['label'] = bn_sent['class_label'].map({'BN_NEG':-1, 'BN_NEU':0, 'BN_POS':1})\n",
        "bn_sent = bn_sent.drop(columns=['class_label', 'id'])\n",
        "bn_sent = bn_sent.sample(frac=1)\n",
        "print(\"Bangla Sentiment Dataset:\")\n",
        "print(bn_sent.head())\n",
        "\n",
        "\n",
        "\n",
        "bn_hate_train = pd.read_csv(f'{data_dir}/bangla/BD-SHS/train.csv')\n",
        "bn_hate_test = pd.read_csv(f'{data_dir}/bangla/BD-SHS/test.csv')\n",
        "bn_hate_val = pd.read_csv(f'{data_dir}/bangla/BD-SHS/val.csv')\n",
        "\n",
        "bn_hate = pd.concat([bn_hate_train, bn_hate_val, bn_hate_test], axis=0, ignore_index=True)\n",
        "bn_hate['label'] = bn_hate['hate speech'].map({1:'HATE', 0:'NOT'})\n",
        "bn_hate.rename(columns={'sentence': 'text'}, inplace=True)\n",
        "bn_hate = bn_hate.drop(columns=['target', 'type', 'hate speech'])\n",
        "bn_hate = bn_hate.sample(frac=1)\n",
        "print(\"\\nBangla Hate Dataset:\")\n",
        "print(bn_hate.head())\n",
        "\n",
        "\n",
        "\n",
        "bn_offn_train = pd.read_json(f'{data_dir}/bangla/HASOC2024/train.json')\n",
        "bn_offn_test = pd.read_json(f'{data_dir}/bangla/HASOC2024/test.json')\n",
        "\n",
        "bn_offn = pd.concat([bn_offn_train, bn_offn_test], axis=0, ignore_index=True)\n",
        "bn_offn['label'] = bn_offn['offensive_gold'].map({'O':'OFFN', 'N':'NOT'})\n",
        "bn_offn = bn_offn.drop(columns=['code_mixed_gold', 'offensive_gold', 'target_gold'])\n",
        "bn_offn = bn_offn.sample(frac=1)\n",
        "print(\"\\nBangla Offensive Dataset:\")\n",
        "print(bn_offn.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2uSGOdM0x-G",
        "outputId": "d19e8c99-d67b-41f9-d0c6-643ef289bc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bangla Sentiment Dataset:\n",
            "                                                  text  label\n",
            "774  '@maimuna_ctg তামিম ফিরে আসছে না রিয়াদ ও আসবে ...      1\n",
            "291  'আসন্ন সিলেট জেলা ছাত্রলীগের কমিটিতে সৎ, যোগ্য...      0\n",
            "483  'বলিউডে বুকে কাঁপন ধরানো পাক সুন্দরীরা: মুম্বা...      0\n",
            "1    'Photo: খুটাখালীতে হেডম্যান পুত্রকে গাছচোর সাজ...     -1\n",
            "506  ' চুমুতে চিনি নেই, মধুও মেশানো থাকে না, তবুও চ...      1\n",
            "\n",
            "Bangla Hate Dataset:\n",
            "                                                    text label\n",
            "35066  ভিডিও টা দেখে মুখের ভাষা হারিয়ে ফেলেছি সত্যিই ...   NOT\n",
            "19351  \"আমি রাজাকার\" লেখা নিয়ে আন্দোলন করলে কবির মনে ...   NOT\n",
            "6645                                আয়োজকরা বলদের বাচ্চা  HATE\n",
            "4149                                  মাগির কোনো খমা নেই  HATE\n",
            "18844                     লুচ্চা সাহতাজ মাগি কাপড় ঠিক কর  HATE\n",
            "\n",
            "Bangla Offensive Dataset:\n",
            "                                                   text label\n",
            "3580  \" Rohinga barmate nirjatito hole ei deshe prot...   NOT\n",
            "684                       Ato paris paki der sathe ja?   OFFN\n",
            "84     acca amr bkash account ta log hoye gece....ko...   NOT\n",
            "1082   ekdom thik r thik ei kothata purush der khetr...   NOT\n",
            "2070              padma pukure tomake onek sundor lage    NOT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - more preprocessing - special characters, emojis\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "  text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "PZCvQUoj1r-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_hate_offn['clean_text'] = mr_hate_offn['text'].apply(clean_text)\n",
        "mr_sent['clean_text'] = mr_sent['text'].apply(clean_text)\n",
        "\n",
        "bn_offn['clean_text'] = bn_offn['text'].apply(clean_text)\n",
        "bn_hate['clean_text'] = bn_hate['text'].apply(clean_text)\n",
        "bn_sent['clean_text'] = bn_sent['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "WK6Bu0X03dR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Marathi Datasets:')\n",
        "print(mr_hate_offn['label'].value_counts())\n",
        "print(mr_sent['label'].value_counts())\n",
        "\n",
        "print('\\nBangla Datasets:')\n",
        "print(bn_hate['label'].value_counts())\n",
        "print(bn_offn['label'].value_counts())\n",
        "print(bn_sent['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMth9RDD4zNd",
        "outputId": "841d19cf-cbe4-4c32-fb50-bd291b06ec69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marathi Datasets:\n",
            "label\n",
            "HATE    6250\n",
            "OFFN    6250\n",
            "PRFN    6250\n",
            "NOT     6250\n",
            "Name: count, dtype: int64\n",
            "label\n",
            " 1    5288\n",
            " 0    5288\n",
            "-1    5288\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Bangla Datasets:\n",
            "label\n",
            "NOT     26125\n",
            "HATE    24156\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "NOT     2619\n",
            "OFFN    2381\n",
            "Name: count, dtype: int64\n",
            "label\n",
            " 0    368\n",
            "-1    353\n",
            " 1    276\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_maps = {\n",
        "  'hate': {'HATE': 1, 'NOT': 0},\n",
        "  'offensive': {'OFFN': 1, 'NOT': 0},\n",
        "  'sentiment': {-1: 0, 0: 1, 1: 2}\n",
        "}"
      ],
      "metadata": {
        "id": "NQe0zOk-ix5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mr_not = mr_hate_offn[mr_hate_offn['label']=='NOT']\n",
        "mr_hate = mr_hate_offn[mr_hate_offn['label']=='HATE']\n",
        "mr_offn = mr_hate_offn[mr_hate_offn['label']=='OFFN']\n",
        "mr_hate_dataset = pd.concat([mr_hate[:3125], mr_not[:3125]], axis=0)\n",
        "mr_offn_dataset = pd.concat([mr_offn[:3125], mr_not[3125:2*3125]], axis=0)\n",
        "\n",
        "mr_sent_neg = mr_sent[mr_sent['label']==-1]\n",
        "mr_sent_neu = mr_sent[mr_sent['label']==0]\n",
        "mr_sent_pos = mr_sent[mr_sent['label']==1]\n",
        "mr_sent_dataset = pd.concat([mr_sent_neg[:4000], mr_sent_neu[:4000], mr_sent_pos[:4000]])\n",
        "\n",
        "bn_hate_dataset = pd.concat([bn_hate[bn_hate['label']=='NOT'][:2750], bn_hate[bn_hate['label']=='HATE'][:2750]])\n",
        "\n",
        "\n",
        "hate_dataset = pd.concat([mr_hate_dataset, bn_hate_dataset]).sample(frac=1)\n",
        "sent_dataset = pd.concat([mr_sent_dataset, bn_sent]).sample(frac=1)\n",
        "offn_dataset = pd.concat([mr_offn_dataset, bn_offn]).sample(frac=1)\n",
        "\n",
        "print(hate_dataset['label'].value_counts())\n",
        "print(sent_dataset['label'].value_counts())\n",
        "print(offn_dataset['label'].value_counts())"
      ],
      "metadata": {
        "id": "tSscUpKf4VZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ded09b9-bd68-4689-ef03-0c4fedd273f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "HATE    5875\n",
            "NOT     5875\n",
            "Name: count, dtype: int64\n",
            "label\n",
            " 0    4368\n",
            "-1    4353\n",
            " 1    4276\n",
            "Name: count, dtype: int64\n",
            "label\n",
            "NOT     5744\n",
            "OFFN    5506\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "  def __init__(self, hate_data, sentiment_data, offensive_data, label_maps, tokenizer):\n",
        "    self.hate_data = hate_data\n",
        "    self.sentiment_data = sentiment_data\n",
        "    self.offensive_data = offensive_data\n",
        "    self.label_maps = label_maps\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = 128\n",
        "    self.data = self.combine_data()\n",
        "\n",
        "  def combine_data(self):\n",
        "    ds_hate = self.hate_data.copy()\n",
        "    ds_hate['task'] = 'hate'\n",
        "    ds_hate = ds_hate.rename(columns={'label': 'task_label'})\n",
        "\n",
        "    ds_offensive = self.offensive_data.copy()\n",
        "    ds_offensive['task'] = 'offensive'\n",
        "    ds_offensive = ds_offensive.rename(columns={'label': 'task_label'})\n",
        "\n",
        "    ds_sentiment = self.sentiment_data.copy()\n",
        "    ds_sentiment['task'] = 'sentiment'\n",
        "    ds_sentiment = ds_sentiment.rename(columns={'label': 'task_label'})\n",
        "\n",
        "    return pd.concat([ds_hate, ds_offensive, ds_sentiment])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.data.iloc[idx]\n",
        "\n",
        "    text = row['clean_text']\n",
        "    task = row['task']\n",
        "    label = row['task_label']\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(text,\n",
        "                                          add_special_tokens=True,\n",
        "                                          max_length=self.max_len,\n",
        "                                          padding='max_length',\n",
        "                                          truncation=True,\n",
        "                                          return_token_type_ids=False,\n",
        "                                          return_attention_mask=True,\n",
        "                                          return_tensors='pt')\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'task': task,\n",
        "        'text': text,\n",
        "        'labels': torch.tensor(self.label_maps[task][label], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eLikJRrXABnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multitask_dataset = MultiTaskDataset(hate_dataset, sent_dataset, offn_dataset, label_maps, tokenizer)\n",
        "train_data, test_data = train_test_split(multitask_dataset.data, test_size=0.1, random_state=42, stratify=multitask_dataset.data[['task', 'task_label']])\n",
        "\n",
        "train_dataset = MultiTaskDataset(\n",
        "  hate_data=train_data[train_data['task']=='hate'],\n",
        "  sentiment_data=train_data[train_data['task']=='sentiment'],\n",
        "  offensive_data=train_data[train_data['task']=='offensive'],\n",
        "  label_maps=label_maps,\n",
        "  tokenizer=tokenizer\n",
        ")\n",
        "print(f\"\\nTrain Dataset:\\n{train_dataset.data['task'].value_counts()}\")\n",
        "\n",
        "\n",
        "test_dataset = MultiTaskDataset(\n",
        "  hate_data=test_data[test_data['task']=='hate'],\n",
        "  sentiment_data=test_data[test_data['task']=='sentiment'],\n",
        "  offensive_data=test_data[test_data['task']=='offensive'],\n",
        "  label_maps=label_maps,\n",
        "  tokenizer=tokenizer\n",
        ")\n",
        "print(f\"\\nTest Dataset:\\n{test_dataset.data['task'].value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWybd3cABT0z",
        "outputId": "bb0f4812-5264-4ebd-924b-c59dcca9d5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Dataset:\n",
            "task\n",
            "sentiment    11697\n",
            "hate         10575\n",
            "offensive    10125\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test Dataset:\n",
            "task\n",
            "sentiment    1300\n",
            "hate         1175\n",
            "offensive    1125\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "A7_sIA65IMjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "  def __init__(self, num_labels_hate, num_labels_offensive, num_labels_sentiment):\n",
        "    super(MultiTaskModel, self).__init__()\n",
        "\n",
        "    self.encoder = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "    hidden_size = self.encoder.config.hidden_size\n",
        "\n",
        "    self.classifier = nn.ModuleDict({\n",
        "      'hate': nn.Linear(hidden_size, num_labels_hate),\n",
        "      'offensive': nn.Linear(hidden_size, num_labels_offensive),\n",
        "      'sentiment': nn.Linear(hidden_size, num_labels_sentiment)\n",
        "    })\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, task):\n",
        "    outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "    cls_output = self.dropout(cls_output)\n",
        "    return self.classifier[task](cls_output)\n"
      ],
      "metadata": {
        "id": "f8xKRABnIoAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiTaskModel(len(label_maps['hate'].keys()), len(label_maps['offensive'].keys()), len(label_maps['sentiment'].keys()))\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "YUbiaoDbIgxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_weight = {}\n",
        "# for task in label_maps.keys():\n",
        "labels_hate = train_dataset.hate_data['task_label'].map(label_maps['hate']).values\n",
        "class_weights_hate = compute_class_weight(class_weight='balanced', classes=np.unique(labels_hate), y=labels_hate)\n",
        "class_weights_hate = torch.tensor(class_weights_hate, dtype=torch.float).to(device)\n",
        "\n",
        "labels_offensive = train_dataset.offensive_data['task_label'].map(label_maps['offensive']).values\n",
        "class_weights_offensive = compute_class_weight(class_weight='balanced', classes=np.unique(labels_offensive), y=labels_offensive)\n",
        "class_weights_offensive = torch.tensor(class_weights_offensive, dtype=torch.float).to(device)\n",
        "\n",
        "labels_sentiment = train_dataset.sentiment_data['task_label'].map(label_maps['sentiment']).values\n",
        "class_weights_sentiment = compute_class_weight(class_weight='balanced', classes=np.unique(labels_sentiment), y=labels_sentiment)\n",
        "class_weights_sentiment = torch.tensor(class_weights_sentiment, dtype=torch.float).to(device)\n"
      ],
      "metadata": {
        "id": "oYhJvhwuMEh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = {\n",
        "  'hate': nn.CrossEntropyLoss(weight=class_weights_hate),\n",
        "  'offensive': nn.CrossEntropyLoss(weight=class_weights_offensive),\n",
        "  'sentiment': nn.CrossEntropyLoss(weight=class_weights_sentiment)\n",
        "}\n",
        "\n",
        "EPOCHS = 5\n",
        "total_steps = EPOCHS * len(train_loader)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=int(0.1*total_steps),\n",
        "  num_training_steps=total_steps\n",
        ")"
      ],
      "metadata": {
        "id": "vr0LcwLtL1os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d20501-c1bd-4f3a-86d9-d3a2c714f86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader, optimizer, device, scheduler, criterion):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  losses = []\n",
        "  for _, batch in enumerate(data_loader):\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    tasks = batch['task']\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    total_loss = 0\n",
        "    for task in set(tasks):\n",
        "      indices = [i for i, t in enumerate(tasks) if t==task]\n",
        "      if not indices:\n",
        "        continue\n",
        "      logits = model(input_ids=input_ids[indices], attention_mask=attention_mask[indices], task=task)\n",
        "      total_loss += criterion[task](logits, labels[indices])\n",
        "\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    losses.append(total_loss.item())\n",
        "\n",
        "    if (_+1)%100==0:\n",
        "      print(f'For Batch {_+1}/{len(data_loader)}, Loss={total_loss.item()}')\n",
        "\n",
        "  return np.mean(losses)\n"
      ],
      "metadata": {
        "id": "pWF6gNaYohyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  all_labels = {'hate': [], 'offensive': [], 'sentiment': []}\n",
        "  all_preds = {'hate': [], 'offensive': [], 'sentiment': []}\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "      tasks = batch['task']\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      total_loss = 0\n",
        "      for task in set(tasks):\n",
        "        indices = [i for i, t in enumerate(tasks) if t==task]\n",
        "        if not indices:\n",
        "          continue\n",
        "        logits = model(input_ids=input_ids[indices], attention_mask=attention_mask[indices], task=task)\n",
        "        total_loss += criterion[task](logits, labels[indices])\n",
        "\n",
        "        all_labels[task].extend(labels[indices].cpu().numpy())\n",
        "        all_preds[task].extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "\n",
        "      losses.append(total_loss.item())\n",
        "\n",
        "    report = {'Precision': {}, 'Recall': {}, 'F1': {}}\n",
        "    for task in all_labels:\n",
        "        report['Precision'][task], report['Recall'][task], report['F1'][task], _ = precision_recall_fscore_support(all_labels[task], all_preds[task], average='weighted', zero_division=0)\n",
        "\n",
        "    return np.mean(losses), report\n",
        "\n"
      ],
      "metadata": {
        "id": "mmlvztMWo9ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = {'hate': 0, 'offensive': 0, 'sentiment': 0}\n",
        "loss_history = {'train': [], 'val': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    train_loss = train_model(model, train_loader, optimizer, device, scheduler, criterion)\n",
        "    loss_history['train'].append(train_loss)\n",
        "    print(f'Train loss: {train_loss}')\n",
        "\n",
        "    val_loss, val_report = eval_model(model, test_loader, device, criterion)\n",
        "    print(f'Validation loss: {val_loss}')\n",
        "    for score_type in val_report.keys():\n",
        "      output_str = f'Validation {score_type}: '\n",
        "      for task in best_score.keys():\n",
        "        output_str += f'{task.title()}: {val_report[score_type][task]}, '\n",
        "      print(output_str)\n",
        "\n",
        "    loss_history['val'].append(val_loss)\n",
        "\n",
        "    torch.save(model.state_dict(), f'{output_dir}/{epoch+1}.pth')\n",
        "    for task in best_score.keys():\n",
        "      if val_report['F1'][task] > best_score[task]:\n",
        "        best_score[task] = val_report['F1'][task]\n",
        "        torch.save(model.state_dict(), f'{output_dir}/best_model_{task}.pth')\n",
        "\n",
        "tokenizer.save_pretrained(f'{output_dir}/tokenizer')\n",
        "torch.save(optimizer.state_dict(), f'{output_dir}/optimizer.pth')\n",
        "torch.save(scheduler.state_dict(), f'{output_dir}/scheduler.pth')\n"
      ],
      "metadata": {
        "id": "5hrEiygupD85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11CQFk_EXZbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "CU4aihpcPM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_revmaps = {\n",
        "  'hate': {1: 'HATE', 0: 'NOT'},\n",
        "  'offensive': {1: 'OFFN', 0: 'NOT'},\n",
        "  'sentiment': {0: -1, 1: 0, 2: 1}\n",
        "}\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "J1vy1CxFPAax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'hate'\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(f'{output_dir}/tokenizer')\n",
        "model.load_state_dict(torch.load(f'{output_dir}/best_model_{task}.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print('Model and tokenizer loaded')"
      ],
      "metadata": {
        "id": "20tngnoePAzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi_train = pd.read_csv(f'{data_dir}/hindi/HASOC2019/hindi_dataset.tsv', sep='\\t')\n",
        "hi_test = pd.read_csv(f'{data_dir}/hindi/HASOC2019/hasoc2019_hi_test_gold_2919.tsv', sep='\\t')\n",
        "\n",
        "hi_combined = pd.concat([hi_train, hi_test], axis=0)\n",
        "hi_combined['label'] = hi_combined['task_1'].map({'NOT': 'NOT', 'HOF': 'HATE'})\n",
        "hi_combined['clean_text'] = hi_combined['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "9qpfm7PgPFPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleTaskDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer):\n",
        "    self.texts = dataframe['clean_text'].tolist()\n",
        "    self.labels = dataframe['label'].tolist()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = 128\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label = self.labels[idx]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(text,\n",
        "                                          add_special_tokens=True,\n",
        "                                          max_length=self.max_len,\n",
        "                                          padding='max_length',\n",
        "                                          truncation=True,\n",
        "                                          return_token_type_ids=False,\n",
        "                                          return_attention_mask=True,\n",
        "                                          return_tensors='pt')\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'text': text,\n",
        "        'label': label\n",
        "    }\n",
        "\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "inference_dataset = SingleTaskDataset(hi_combined, tokenizer)\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "FQnH_rTNH2rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "with torch.no_grad():\n",
        "  for batch in inference_loader:\n",
        "    logits = model(input_ids=batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), task=task)\n",
        "    predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "hi_combined['hate_prediction'] = predictions\n",
        "hi_combined['hate_prediction'] = hi_combined['hate_prediction'].map(label_revmaps[task])\n",
        "\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(5):\n",
        "  print(f\"\\nText: {hi_combined['text'].iloc[i]}\")\n",
        "  print(f\"Clean Text: {hi_combined['clean_text'].iloc[i]}\")\n",
        "  print(f\"True Label: {hi_combined['label'].iloc[i]}\")\n",
        "  print(f\"{task.title()} Prediction: {hi_combined['hate_prediction'].iloc[i]}\")\n",
        "\n",
        "print(f\"\\nClassification Report for {task}:\\n{classification_report(hi_combined['label'], hi_combined['hate_prediction'], zero_division=0)}\")\n"
      ],
      "metadata": {
        "id": "IS2xipw5PTrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4Er6Zh1WR1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References:**\n",
        "1.   https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation\n",
        "2. https://www.analyticsvidhya.com/blog/2023/06/step-by-step-bert-implementation-guide/\n",
        "3. https://discuss.pytorch.org/t/dealing-with-imbalanced-datasets-in-pytorch/22596/5\n",
        "4. https://medium.com/analytics-vidhya/pre-processing-tweets-for-sentiment-analysis-a74deda9993e\n",
        "5. https://medium.com/gumgum-tech/an-easy-recipe-for-multi-task-learning-in-pytorch-that-you-can-do-at-home-1e529a8dfb7f\n",
        "6. https://stackoverflow.com/questions/57416925/best-practices-for-generating-a-random-seeds-to-seed-pytorch\n",
        "\n",
        "**Datasets:**\n",
        "1. https://hasocfire.github.io/hasoc/2024/call_for_participation.html\n",
        "2. https://www.kaggle.com/datasets/naurosromim/bdshs\n",
        "3. https://github.com/banglanlp/bnlp-resources/tree/main/sentiment\n",
        "4. https://github.com/l3cube-pune/MarathiNLP/tree/main\n"
      ],
      "metadata": {
        "id": "a4L5ATL5l5yr"
      }
    }
  ]
}